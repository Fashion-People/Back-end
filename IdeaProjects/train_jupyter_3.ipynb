{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # device 객체\n",
    "print(device)\n",
    "\n",
    "#print(fm.findSystemFonts(fontpaths=None, fontext='ttf'))\n",
    "\n",
    "fontpath = 'C:/Windows/Fonts/NanumGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=10).get_name()\n",
    "plt.rc('font', family=font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(), # 데이터 증진(augmentation)\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # 정규화(normalization)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='C:/Users/kangb/capstoneDesign/Back-end/IdeaProjects/imageDownloader/Image'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = datasets.ImageFolder(os.path.join(data_dir, 'train2'), transforms_train)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_datasets, batch_size=17, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터셋 크기: 2602\n",
      "학습 클래스: ['가디건', '긴팔 티', '누빔 옷', '니트', '린넨 옷', '맨투맨', '민소매', '반팔', '블라우스', '야상', '얇은 셔츠', '자켓', '청자켓', '코트', '트렌치코트', '패딩', '후드티']\n"
     ]
    }
   ],
   "source": [
    "print('학습 데이터셋 크기:', len(train_datasets))\n",
    "#print('테스트 데이터셋 크기:', len(valid_datasets))\n",
    "\n",
    "class_names = train_datasets.classes\n",
    "print('학습 클래스:', class_names)\n",
    "\n",
    "def imshow(input, title):\n",
    "    # torch.Tensor를 numpy 객체로 변환\n",
    "    input = input.numpy().transpose((1, 2, 0))\n",
    "    # 이미지 정규화 해제하기\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    input = std * input + mean\n",
    "    input = np.clip(input, 0, 1)\n",
    "    # 이미지 출력\n",
    "    plt.imshow(input)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 경로\n",
    "model_path = './weight/model_best_epoch2.pth'\n",
    "\n",
    "# 모델 로드\n",
    "loaded_model = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'in_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_29700/2516898198.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mloaded_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnum_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# 전이 학습(transfer learning): 모델의 출력 뉴런 수를 3개로 교체하여 마지막 레이어 다시 학습\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# transfer learning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m model.fc = nn.Sequential(     \n",
      "\u001b[1;32mc:\\Users\\kangb\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1183\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1184\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1185\u001b[1;33m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[0;32m   1186\u001b[0m             type(self).__name__, name))\n\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'in_features'"
     ]
    }
   ],
   "source": [
    "model =loaded_model\n",
    "num_features = model.fc.in_features\n",
    "# 전이 학습(transfer learning): 모델의 출력 뉴런 수를 3개로 교체하여 마지막 레이어 다시 학습\n",
    "# transfer learning\n",
    "model.fc = nn.Sequential(     \n",
    "    nn.Linear(num_features, 256),        # 마지막 완전히 연결된 계층에 대한 입력은 선형 계층, 256개의 출력값을 가짐\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(256, num_features),      # Since 10 possible outputs = 10 classes\n",
    "    nn.LogSoftmax(dim=1)              # For using NLLLoss()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-5)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "best_epoch = None\n",
    "best_loss = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 Loss: 0.2258 Acc: 92.7748% Time: 1180.2192s\n",
      "best_loss: 0.2258 \t best_epoch: 0\n",
      "#1 Loss: 0.1956 Acc: 93.4666% Time: 1188.9751s\n",
      "best_loss: 0.1956 \t best_epoch: 1\n",
      "#2 Loss: 0.1954 Acc: 94.1583% Time: 1203.8134s\n",
      "best_loss: 0.1954 \t best_epoch: 2\n",
      "#3 Loss: 0.1722 Acc: 95.0038% Time: 1199.4506s\n",
      "best_loss: 0.1722 \t best_epoch: 3\n",
      "#4 Loss: 0.1661 Acc: 94.8117% Time: 1203.3256s\n",
      "best_loss: 0.1661 \t best_epoch: 4\n",
      "#5 Loss: 0.1421 Acc: 96.2721% Time: 1197.9305s\n",
      "best_loss: 0.1421 \t best_epoch: 5\n",
      "#6 Loss: 0.1344 Acc: 95.6956% Time: 1207.8289s\n",
      "best_loss: 0.1344 \t best_epoch: 6\n",
      "#7 Loss: 0.1004 Acc: 96.7717% Time: 1275.1534s\n",
      "best_loss: 0.1004 \t best_epoch: 7\n",
      "#8 Loss: 0.1036 Acc: 96.9254% Time: 1433.1559s\n",
      "#9 Loss: 0.0992 Acc: 96.8101% Time: 1411.0033s\n",
      "best_loss: 0.0992 \t best_epoch: 9\n",
      "#10 Loss: 0.1156 Acc: 96.3874% Time: 1392.6176s\n",
      "#11 Loss: 0.0925 Acc: 96.9639% Time: 1253.0995s\n",
      "best_loss: 0.0925 \t best_epoch: 11\n",
      "#12 Loss: 0.1101 Acc: 96.5411% Time: 1207.3330s\n",
      "#13 Loss: 0.0843 Acc: 97.1560% Time: 1202.0752s\n",
      "best_loss: 0.0843 \t best_epoch: 13\n",
      "#14 Loss: 0.1041 Acc: 96.8870% Time: 1196.6738s\n",
      "#15 Loss: 0.0741 Acc: 97.5019% Time: 1195.0568s\n",
      "best_loss: 0.0741 \t best_epoch: 15\n",
      "#16 Loss: 0.0947 Acc: 96.6564% Time: 1194.1566s\n",
      "#17 Loss: 0.0906 Acc: 97.0792% Time: 1222.5141s\n",
      "#18 Loss: 0.0780 Acc: 97.3482% Time: 1205.2626s\n",
      "#19 Loss: 0.0707 Acc: 97.5019% Time: 1202.6549s\n",
      "best_loss: 0.0707 \t best_epoch: 19\n",
      "#20 Loss: 0.0815 Acc: 97.0023% Time: 1208.2200s\n",
      "#21 Loss: 0.0689 Acc: 97.4251% Time: 1205.1930s\n",
      "best_loss: 0.0689 \t best_epoch: 21\n",
      "#22 Loss: 0.0782 Acc: 97.1945% Time: 1198.9245s\n",
      "#23 Loss: 0.0757 Acc: 97.1560% Time: 1198.0741s\n",
      "#24 Loss: 0.0683 Acc: 97.7325% Time: 1200.3582s\n",
      "best_loss: 0.0683 \t best_epoch: 24\n",
      "#25 Loss: 0.0562 Acc: 98.1553% Time: 1197.6950s\n",
      "best_loss: 0.0562 \t best_epoch: 25\n",
      "#26 Loss: 0.0691 Acc: 97.5788% Time: 1232.2783s\n",
      "#27 Loss: 0.0602 Acc: 97.9631% Time: 1187.9715s\n",
      "#28 Loss: 0.0616 Acc: 97.5404% Time: 1190.5720s\n",
      "#29 Loss: 0.0531 Acc: 97.8862% Time: 1199.5747s\n",
      "best_loss: 0.0531 \t best_epoch: 29\n"
     ]
    }
   ],
   "source": [
    "''' Train '''\n",
    "# 전체 반복(epoch) 수 만큼 반복하며\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    running_loss = 0.\n",
    "    running_corrects = 0\n",
    "\n",
    "    # 배치 단위로 학습 데이터 불러오기\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # 모델에 입력(forward)하고 결과 계산\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 역전파를 통해 기울기(gradient) 계산 및 학습 진행\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_datasets)\n",
    "    epoch_acc = running_corrects / len(train_datasets) * 100.\n",
    "\n",
    "    # 학습 과정 중에 결과 출력\n",
    "    print('#{} Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))\n",
    "\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        best_epoch = epoch\n",
    "        print(\"best_loss: {:.4f} \\t best_epoch: {}\".format(best_loss, best_epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./weight',exist_ok=True)\n",
    "torch.save(model,'./weight/model_best_epoch3.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
